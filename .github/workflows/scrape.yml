name: Daily OLX Dacha Scraper

on:
  push:            # First run when you upload/push
    branches:
      - "**"
  schedule:        # Daily at 10:00 Asia/Tashkent (UTC+5) = 05:00 UTC
    - cron: "0 5 * * *"
  workflow_dispatch: {}
  
concurrency:
  group: scraper
  cancel-in-progress: true
  
permissions:
  contents: write  # commit last_run.txt, runs_log.csv, state.json

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    env:
      TZ: Asia/Tashkent
      SERVICE_ACCOUNT_JSON: ${{ secrets.SERVICE_ACCOUNT_JSON }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install Python deps
        run: |
          python -m pip install -U pip
          pip install -r Project/requirements.txt

      - name: Install Playwright (browsers + libs)
        run: |
          python -m playwright install --with-deps

      - name: Restore service account file
        shell: bash
        run: |
          set -euo pipefail
          PD="${{ steps.detect.outputs.dir }}"
          if [ -z "$PD" ]; then PD="."; fi
          echo "Project dir resolved to: $PD"
          mkdir -p "$PD"

          # Write the secret exactly as-is into the JSON file
          printf '%s' "${SERVICE_ACCOUNT_JSON}" > "$PD/dacha-data-scraping-bc5665b6482e.json"

          # Quick sanity check (size only; content stays secret)
          ls -l "$PD/dacha-data-scraping-bc5665b6482e.json"

      - name: Validate service account JSON
        run: |
          # Make PD available to child processes (Python)
          export PD="${{ steps.detect.outputs.dir }}"
          python - <<'PY'
      - name: Mark start time (Tashkent)
        run: |
          echo "START_TS=$(date +%s)" >> $GITHUB_ENV
          echo "START_HUMAN=$(TZ=Asia/Tashkent date '+%Y-%m-%d %H:%M:%S %Z')" >> $GITHUB_ENV

      - name: Run scraper
        working-directory: Project
        run: |
          python scrape_olx_dacha_tashkent.py

      - name: Capture job status for logging
        if: ${{ always() }}
        run: |
          echo "JOB_STATUS=${{ job.status }}" >> $GITHUB_ENV

      - name: Write last_run.txt and append runs_log.csv
        if: ${{ always() }}
        run: |
          END_TS=$(date +%s)
          END_HUMAN=$(TZ=Asia/Tashkent date '+%Y-%m-%d %H:%M:%S %Z')
          DURATION=$((END_TS - START_TS))
          H=$((DURATION/3600)); M=$(((DURATION%3600)/60)); S=$((DURATION%60))
          DURATION_HMS=$(printf "%02d:%02d:%02d" "$H" "$M" "$S")

          # Overwrite summary file
          printf "start:   %s\nend:     %s\nduration_seconds: %s\nduration_hms: %s\nstatus:  %s\n" \
            "$START_HUMAN" "$END_HUMAN" "$DURATION" "$DURATION_HMS" "$JOB_STATUS" > Project/last_run.txt

          # Append CSV log with header-once
          LOG=Project/runs_log.csv
          if [ ! -f "$LOG" ]; then
            echo "date_local,start_local,end_local,duration_seconds,duration_hms,run_id,run_number,sha,status" > "$LOG"
          fi
          SHORT_SHA=$(echo "$GITHUB_SHA" | cut -c1-8)
          DATE_LOCAL=$(TZ=Asia/Tashkent date '+%Y-%m-%d')
          printf "%s,%s,%s,%s,%s,%s,%s,%s,%s\n" \
            "$DATE_LOCAL" "$START_HUMAN" "$END_HUMAN" "$DURATION" "$DURATION_HMS" \
            "$GITHUB_RUN_ID" "$GITHUB_RUN_NUMBER" "$SHORT_SHA" "$JOB_STATUS" >> "$LOG"

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: olx-dacha-csv
          path: Project/olx_dacha_tashkent_raw_*.csv
          if-no-files-found: ignore

      - name: Commit updated state.json, last_run.txt, runs_log.csv
        if: ${{ always() }}
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add Project/state.json Project/last_run.txt Project/runs_log.csv
          git commit -m "Update state, last_run, runs_log [skip ci]" || echo "No changes"
          git push
